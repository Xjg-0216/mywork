#### 损失函数



##### 1、交叉熵损失函数

* 经过神经网络最后一层得到每个类别的得分`score`
* 该得分经过`sigmoid`（二分类）或者 `softmax`（多分类）获得概率输出
* 模型预测的类别概率输出与真实类别的one hot形式进行交叉熵损失函数的计算



二分类：

在二分的情况下，模型最后要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为p与1-p，

此时的损失为：
$$
L = \frac{1}{N}\sum_{i}[ylog(p) + (1-y)log(1-p)]
$$
其中，y表示样本i的真实标签，p表示样本i的预测值，N代表样本容量

```python
import numpy as np

def cross_entropy(y, p):
    y = np.float_(y)
    p = np.float_(p)
    return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p)) /len(y)
```

利用**BCELoss**进行损失计算

```python
import torch
import torch.nn as nn
model = nn.Sequential(nn.Linear(10, 1), nn.Sigmoid())
criterion = nn.BCELoss()
x = torch.randn(16, 10) #shape: (16, 10)
y = torch.empty(16).random_(2) #shape: (16,) 二分类任务
out = model(x)
out = out.squeeze(dim = 1)# shape: (16,)
loss = criterion(out, y)
```

多分类
$$
Loss = - \sum_{i = 0}^{N-1}y_ilogp_i
$$
$y_i$判断样本$i$是否为真实类别，是等于1， 否则等于0， $p_i$为样本$i$属于真实类别的预测概率

```python
import torch
import torch.nn as nn
model = nn.Sequential(nn.Linear(10, 3), nn.LogSoftmax())
criterion =  nn.NLLLoss()
x = torch.randn(16, 10)
y = torch.randint(0, 3, size = (16, )) #3类， shape:(16, )
out = model(x)  #shape: (16, 3)
loss = criterion (out, y)
```

##### 2、focal loss

```python
def py_sigmoid_focal_loss(pred, target, weight=None, gamma=2.0, alpha=0.25, reduction, ):
    pred_sigmoid = pred.sigmoid()
    target = target.type_as(pred)
    pt = (1 - pred_sigmoid)*target + pred_sigmoid*(1-target)
    focal_weight = (alpha * target + (1 - alpha) * (1 - target) )* pt.pow(gamma)
    loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none') * focal_weight
    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)
    return loss
```

