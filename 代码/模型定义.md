####  模型构造：

```python
import torch.nn as nn

class Mymodel(nn.Module):
   def __init__(self):
     super(Mymodel, self).__init__()
     self.conv1 = nn.Conv2d(inputchannel, outchannel, kernelsize, stride, padding)
    
   def forward(self, x):
      x = self.conv1(x)
      return x
net = Mymodel()
output = net(input)
```

`实例化后为什么不用net.forward(input)`？

`nn.Module`中的方法中有`__call__`会帮你调用`forward`

**MLP网络**

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self, *args):
        super().__init__()
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)
    def forward(self, x):
        x = self.linear(x)
        x = F.relu(torch.mm(x, self.rand_weight) + 1)
        x = self.linear(x)
        while x.abs().sum() > 1:
            x /= 2
        return x.sum()
net = FixesHiddenMLP()
net(x)
```

`ModuleList`是`nn.module`中的子类，因此也会继承这个方法

```python
class Linear(nn.Module):
    def __init__(self, input_size, num_layers, layers_size, output_size):
        super(Linear, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(inputsize, outputsize) for i in range(1, self.nums_layer - 1)])
        
    def forward(self, input):
        for model in self.linears:
            input = model(input)
```



`torch.nn.Sequential`是一个Sequential容器，模块将按照构造函数中传递的**顺序**添加到模块中。另外，也可以传入一个有序模块，

```python
model = nn.Sequential(nn.Conv2d(1, 20, 5), nn.ReLU(), nn.Conv2d(20, 24, 5), nn.ReLU)

model = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20, 5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20, 64, 5)), ('relu2', nn.ReLU())]))
```

#### **参数访问**

`model[0]`是卷积层，可以通过`model[0].state_dict()`把这一层的包括`weight`和`bias`拿出来

也可以通过`model[0].weight`或者`model[0].bias`直接索引， 但是由于包含梯度信息， 所以我们可以通过`model[0].weight.data`查看， 通过`model[0].weight.grad`访问它的梯度信息。

**一次性访问所有参数**

```python
for name, param in model.named_parameters():
    print((name, param))
```

#### 模型初始化

```python
def init_norm(m):
    if isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, mean=0, std= 0.01)
        #下划线的作用是原地置换
        nn.init.zeros_(m.bias)
net.apply(init_norm)

def init_constant(m):
    if isinstance(m, nn.Linear):
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)
```

**对某些块应用不同的初始化方法**

```python
def xavier(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if isinstance(m, nn.Linear):
        nn.init.constant_(m.weight, 42)
net[0].apply(xavier)
net[2].apply(init_42)
```

**整个网络进行初始化**

```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    def forward(self, x):
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.rlu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight.data)
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                torch.nn.init.constant_(m.weight, 1)
                torch.nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                torch.nn.init.normal_(m.weight, 0, 0.01)
                torch.nn.init.zeros_(m.bias)
net = Net()
net.initialize_weights()            
```

#### 自定义层

构造一个没有任何参数的自定义层

```python
import torch
import torch.nn.functional as F
from torch import nn
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        return x - x.mean()
layer = CenteredLayer()
out = layer(torch.FloatTensor([1, 2, 3, 4, 5]))
```

**带参数的图层**

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units, ))
    def forward(self, x):
        linear = torch.matmul(x, self.weight.data) + self.bias.data
        return F.relu(linear)
dense = MyLinear(5, 3)
dense.weight
```

**读写文件**

模型保存：

```python
torch.save(net, 'net.pkl')    #保存整个网络模型及参数
torch.save(net.state_dict(), 'net_params.pkl')  # 只保存模型的参数
```

模型加载：

```python
torch.load('model.pth') # 加载模型
model_object.load_state_dict(torch.load('param.pth')) #  仅加载模型参数
```

