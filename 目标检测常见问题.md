#### 目标检测常见问题

[TOC]



##### 1、手写IoU

```python
### IoU
def compute_iou(box1, box2):
    """
    box1 : (x0, y0, x1, y1)
    box2 : (x0, y0, x1, y1)
    """
    x1, y1, x2, y2 = box1
    x3, y3, x4, y4 = box2
    s_box1 = (y2 - y1) * (x2 - x1)
    s_box2 = (y4 - y3) * (x4 - x3)
    left = max(y1, y3)
    top = max(x1, x3)
    right = min(y2, y4)
    bottom = min(x2, x4)
    if left >= right or top >= bottom:
        return 0
    else:
        intersect = (right - left) * (bottom - top)
        area_union = s_box1 + s_box2 - intersect
        return intersect / area_union
    
### GIou
def compute_iou(box1, box2):
    x1, y1, x2, y2 = box1
    x3, y3, x4, y4 = box2
    s_box1 = (y2 - y1)*(x2 - x1)
    s_box2 = (y4 - y2)*(x4 - x2)
    left = max(y1, y3)
    top = max(x1, x3)
    right = min(y2, y4)
    bottom = min(x2, x4)
    if left >= right or top >= bottom:
        return 0
    else:
        intersect = (right - left)*(bottom - top)
        area_union = s_box1 + s_box2 - intersect
        area_c = (max(x2, x4) - min(x1, x3)) *(max(y2, y4) - min(y1, y3))    
        Giou = intersect / area_union + (area_c - area_union) / area_c
        return Giou
    
### DIoU
def compute_iou(box1, box2):
    x1, y1, x2, y2 = box1
    x3, y3, x4, y4 = box2
    s_box1 = (y2 - y1)*(x2 - x1)
    s_box2 = (y4 - y3)*(x4 - x3)
    left = max(x1, x3)
    top = max(y1, y3)
    right = min(x2, x4)
    bottom = min(y2, y4)
    center_x1 = (x2 - x1) / 2
    center_y1 = (y2 - y1) / 2
    center_x2 = (x4 - x3) / 2
    center_y2 = (y4 - y3) / 2
    d_c = ((center_x1 - center_x2)**2 + (center_y1 - center_y2)** 2)
    width = max(x2, x4) - min(x1, x3)
    height = max(y2, y4) - min(y1, y3)
    diag_c = (width**2 + height**2)
    if left >= right or top >= bottom:
        return 0
    else:
        insersect = (bottom - top) * (right - left)
        area_union = s_box1 + s_box2 - insersect
        iou = insersect / area_union
        diou = iou - d_c / diag_c
### CIoU
def compute_iou(box1, box2):
    x1, y1, x2, y2 = box1
    x3, y3, x4, y4 = box2
    left = max(x1, x3)
    top = max(y1, y3)
    right = min(x2, x4)
    bottom = min(y2, y4)
    center_x1, center_y1 = x2 - x1, y2 - y1 
    center_x2, center_y2 = x4 - x3, y4 - y3
    center_dis = (center_x1 - center_x2)**2 +(center_y1 - center_y2)**2
    c_dis = (max(x2, x4) - min(x1, x3))**2 + (max(y2, y4) - min(y1, y3))**2
    arc = (acrtan((x2 - x1) / (y2 - y1))-arctan((x4 - x3) / (y4 - y3)))**2
    v = 4 / pi * arc
    intersect = (right - left)*(bottom - top)
    area_1 = (x2 - x1)*(y2 - y1)
    area_2 = (x4 - x3)*(y4 - y3)
    iou = area_1 + area_2 -intersect
    alpha = v / (1 - iou + v)
    return iou - center_dis / c_dis + alpha * v
```



##### 2、手写NMS

算法实现思路：

先对每个框的score进行排序，首先选择score最高的框

然后与其他框进行比较，当iou大于一定的阈值， 说明两者的重合度高，应该去掉，这样筛选出的框就是和第一框重合度低的框，第一次迭代结束；第二次还是选择score最高的，重复上述过程直到没有框

```python
def nms(dets, thresh):
    #dets: x1, y1, x2, y2, score
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    score = dets[:, 4]
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = score.argsort()[::-1]
    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[1:])
        yy1 = np.maximum(y1[i], y1[1:])
        xx2 = np.minimum(x2[i], x2[1:])
        yy2 = np.minimum(y2[i], y2;[1:])
        w = np.maximum(0, xx2 - xx1)
        h = np.maximum(0, yy2 - yy1)
        inter = w * h
        iou = inter / (areas[i] + areas[1:] - inter)
        inds = np.where(iou < thresh)[0]
        order = order[inds + 1]
    return keep  
```



##### 3、对比YOLOv1和YOLOv3





##### 4、L1、L2范数

正则化是机器学习中对原始损失函数引入惩罚项， 以防止过拟合或提高模型泛化性能的一类方法的总称，常见的正则化有两种，L1范数和L2范数

* L1范数是各个参数的绝对值之和
* L2范数是模型各个参数的平方和开根号

L1范数会使得较多的模型参数为0， 从而产生稀疏解，我们可以将0对应的特征遗弃，进而用来进行特征选择，一定程度上可以防止模型的过拟合，当L1的正则化系数很小时， 得到的最优解会很小，可以达到和L2正则化类似的效果。

L2范数通过权重衰减，可以使得权重趋向于0， 但不为0， 导致模型权重参数较小且较为平滑，防止模型过拟合。

L2范数的效果是对原最优解的每个元素进行不同比例的放缩；L1范数则会使得原最优解的元素产生不同量的偏移，并使某些元素为0， 从而产生稀疏性。





##### 5、SmoothL1 loss相比L1 loss和L2 loss的优势在哪里？这些loss的公式

为了从两方面限制梯度：

1、当预测框与gt差别过大时，梯度值不至于过大；

2、当预测框与gt差别很小时，梯度值足够小；

考察如下几种损失函数，其中x为预测框与gt之间elementwise的差异：
$$
L_2(x) = x^2
$$

$$
L_1(x) = |x|
$$

$$
smooth_{L1}(x) = 
\left\{
\begin{matrix}
0.5x^2,   if |x| < 1 \\
|x| - 0.5 , otherwise\\
\end{matrix}
\right.
$$

损失函数对x的导数分别为：
$$
\frac{dL_2(x)}{dx} = 2x
$$

$$
\frac{dL_1(x)}{dx} = 
\left\{
\begin{matrix}
1, if x >= 0\\
-1, othersize\\
\end{matrix}
\right.
$$

$$
\frac{d smooth_{L1}}{dx}=
\left\{
\begin{matrix}
x, if |x| < 1\\
1 or -1, otherwise\\
\end{matrix}
\right.
$$

(4)： 当x增大时，L2损失对x的导数很大，这就导致**训练初期**， 预测值与gt差异过大，梯度十分大，训练不稳定

(5)： L1对x的导数为常数， 这就导致**训练后期**，预测值与gt过小时，梯度的决定于值仍为1， 损失函数将在稳定值附近，难以继续收敛以达到更高的精度。

##### 6、有没有看过anchor free的检测框架

##### 7、小目标如何改进

##### 8、对YOLOv4中的CSP结构有无了解

##### 9、mixup的优势及权值的影响？



##### 10、1*1卷积优势

1、降维和升维

在网络中添加1*1的卷积使得网络更深，但并没有增加权重参数的负担，反而大大减少。

2、跨通道信息交互

通道间信息的线性组合变化

3、增加非线性特性

可以在保持feature map 大小不变的前提下大幅增加非线性特性。（利用后面的非线性激活函数）



##### 11、ROI pooling的过程

##### 12、ROI align的改进

##### 13、一阶段和两阶段的区别

##### 14、了解的数据增强策略

##### 15、yolov4和yolov5的具体改进的点

##### 16、anchor-based和anchor-free的区别

##### 17、旋转框检测框和水平框的区别

##### 18、常见的分类损失和回归损失

##### 19、focal loss中的参数，哪个关注难样本，哪个解决长尾问题

##### 20、如果label中有错误标签，我们却不知道，怎么解决

##### 21、anchor怎么设置，不同网络anchor设置的差别（SSD, fasterRCNN, yolov3）

##### 22、BN设置时是每个batch进行计算，但之前的计算资源比较匮乏，batch很小，所以BN计算的时候特别敏感，会震荡不能很好收敛解决方法？

每隔几个batch做BN

