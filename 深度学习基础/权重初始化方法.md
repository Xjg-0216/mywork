### 权重初始化方法

神经网络的权重初始化方法对模型的收敛速度和性能有着至关重要的影响。一个好的权重初始化虽然不能完全解决梯度消失和梯度爆炸问题， 但是对于处理这两个问题是有很大帮助的， 并且十分有利于模型性能和收敛速度。

常见的权重初始化方法有5种：

* 权重初始化为0
* 权重随机初始化
* $Xavier$ $initialization$
* $He$ $initialization$
* 预训练权重



#### 1、权重初始化为0

如果将权重初始化全部为$0$的话， 这样的操作等同于等价于一个线性模型， 将所有权重设置为$0$时， 对于每一个$w$而言， 损失函数的导数都是相同的， 因此在随后的迭代过程中所有权重都具有相同的值， 这会使得隐藏单元变得对称， 并继续运行设置的`n`次的迭代， 会导致网络中同一神经元的不同权重都是一样的， 下面代码为权重初始化为$0$的代码：

```python
def initialize_parameters_zeros(layers_dims):
    parameters = {}
    np.random.seed(3)
    L = len(layers_dims)
    for l in range(1, L):
        parameters['w' +str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))
        parameters['b' +str(l)] = np.zeros((layers_dims[l], 1))
    return parameters
```

#### 2、权重随机初始化

```python
def initialize_parameters_random(layers_dims):
    parameters = {}
    np.random.seed(3)
    L = len(layers_dims)
    for l in range(1, L):
        parameters['w' + str(l)] = np.random.rand(layers_dims[l], layers_dims[l-1]) *0.01
        parameters['b' + str(l)] = np.zeros(layers_dims[l], 1)
```

上述代码中权重乘$0.01$是因为要把$W$随机初始化到一个相对较小的值，因为如果$X$很大的话，$W$又相对较大，会导致$Z$非常大，这样如果激活函数是$sigmoid$，就会导致$sigmoid$的输出值$1$或者$0$，然后会导致一系列问题（比如$cost$ $function$计算的时候，$log$里是$0$，这样会有点麻烦）

#### 3、Xavier initialization

在使用以上两种方法来初始化权重极易出现梯度消失的问题， 而$Xavier initialization$出现就解决了上面的问题。其思想就是尽可能的让输入和输出服从相同的分布， 这样就能够避免后面层的激活函数的输出值趋向于0

我们来简单推导一下$Xavier$初始化的原理：首先我们定义一层的卷积运算为如下公式，其中${n_i}$表示输入的个数。

$$
y = w_1x_1 + ··· + w_{ni}x_{ni}+ b
$$

根据我们学过的概率论统计知识可以得到如下的方差公式：

$$
Var(w_ix_i)=E[w_i]^2Var(x_i) + E[x_i]^2Var(w_i) + Var(w_i)Var(x_i)
$$

当我们假设输入和输入的权重的均值都是$0$(使用BN层以后很容易就可以满足)时，上式可以简化为：

$$
Var(w_ix_i)=Var(w_i)Var(x_i)
$$

进一步我们假设输入$x$和权重$w$都是独立同分布，则可以得到：

$$
Var(y) = n_iVar(w_i)Var(x_i)
$$

于是按照$Xavier$的要求保证输入和输出的方差要一致，则可以得到：

$$
Var(w_i) = \frac{1}{n_i}
$$

对于一个多层的网络，某一层的方差可以用累积的形式表达：

$$
Var[z^i] = Var[x]\prod_{i^{`}=0}^{i-1}n_{i^`}Var[W^{i^`}]
$$

对于误差反向传播也有类似的表达形式，如下所示，其中$n_{i+1}$表示输出个数

$$
Var[\frac{\partial Cost}{\partial s^i}] = Var[\frac{\partial Cost}{\partial s^d}]\prod_{i^{`}=i}^{d}n_{i^`+1}Var[W^{i^`}]
$$

综上，为了保证前向传播和反向传播时每一层的方差一致，应满足：

$$
\forall_i ，n_iVar[W^i]=1
$$

$$
\forall_i ，n_{i+1}Var[W^i]=1
$$

但是，实际当中输入与输出的个数往往不相等，于是为了均衡考量输出和输入，最终我们的权重的方差应满足如下要求：

$$
\forall_i ，Var[W^i]= \frac{2}{n_i + n_{i+1}}
$$

##### $Xavier$均匀分布初始化

```python
torch.nn.init.xavier_uniform(tensor, gain=1)
#tensor表示要初始化的张量， gain表示缩放因子
```

##### $Xavier$正态分布初始化

```python
torch.nn.init.xavier_normal(tensor, gain=1)
```

由此可见，**$Xavier$权重初始化方式比较适用于$tanH$和$Sigmoid$激活函数**，而对于$ReLU$这种非对称性的激活函数还是容易出现梯度消失的现象。。

#### 4、He initializaiton

$He$ $initialization$是由何凯明大神提出的一种针对$ReLU$激活函数的初始化方法。

$He$ $initialization$的思想是：和$Xavier$初始化方式一样，都希望初始化使得正向传播时，状态值的方差保持不变，反向传播时，关于激活值的梯度的方差保持不变。由于小于$0$的值经过$ReLU$激活函数都会变成$0$，而大于$0$的值则保持原值。因此在$ReLU$网络中，假定每一层有一半的神经元被激活，另一半为$0$，所以，要保持$variance$不变，只需要在$Xavier$的基础上再除以2即可。

##### $He$ $initialization$均匀分布初始化

```python
torch.nn.init.kaiming_uniform(tensor, a = 0, mode = 'fan_in')
#a表示这层之后使用的rectifier的斜率稀疏
#mode中的'fan_in'表示保留前向传播时权值方差的量级。'fan_out'保留反向传播时的量级
```

##### $He$ $initialization$正态分布初始化

```python
torch.nn.init.kaiming_normal(tensor, a=0, mode='fan_in')
```

#### 总结

1、权重采用初始化为$0$和随机初始化都比较容易出现梯度消失的问题，因此不常用。

2、$Xavier$权重初始化方式主要针对于$tanH$和$sigmoid$激活函数。

3、$He$ $initialization$权重初始化方式主要针对于$ReLU$激活函数。

#### pytorch

`pytorch`中参数的默认初始化在各个层的`reset_parameters()`方法中。都是在`[-l, l]`之间的均匀分布， 其中`l`是 `1./ sqrt(fan_in) `， `fan_in `表示参数张量的输入单元的数量。

```python
def weights_init(model_list):
    for m in module_list:
    	if isinstance(m, nn.Conv2d):
            init.kaiming_normal_(m.weight)
            if m.bias is not None:
                m.bias.data.fill_(0)
        elif isinstance(m, nn.BatchNorm2d):
            m.eps = 1e-3
            m.momentum = 0.03
        elif isinstance(m, nn.LeakReLU):
            m.isplace = True     
```



