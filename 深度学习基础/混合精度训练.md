### 混合精度训练`(torch.cuda.amp)`

`torch`默认数据是`float32`， 也就是单精度训练

```python
x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

optimizer = torch.optim.Adam(self.parameters(), lr= self.lr)
scheduler = torch.optim.lr_schecher.OneCycleLR(
			optimizer, self.lr, cycle_momentum=False, epochs=self.epochs, steps_per_epoch=int(np.ceil(len(x)/self.batch_size)))
batches = torch.utils.data.DataLoader(torch.util.data.TensorDataset(x, y), batch_size=self.batch_size, shuffle=True)
for epoch in range(self.epochs):
    for i, (x_batch, y_batch) in enumerate(batches):
        x_batch = x_batch.cuda()
        y_batch = y_batch.cuda()
        optimizer.zero_grad()
        y_pred = model(x_batch).squeeze()
        loss = self.loss_fn(y_pred, y_batch)
        loss.backward()
        lv = loss.detach().cpu().numpy()
        print(f"Epoch {epoch+1}/{self.epoch}; Batch {i}; Loss: {lv}")
        optimizer.step()
        scheduler.step()
```



通过`torch.cuda.amp`进行混合精度

```python
x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

optimizer = torch.optim.Adam(self.parameters(), lr= self.lr)
scheduler = torch.optim.lr_schecher.OneCycleLR(
			optimizer, self.lr, cycle_momentum=False, epochs=self.epochs, steps_per_epoch=int(np.ceil(len(x)/self.batch_size)))
batches = torch.utils.data.DataLoader(torch.util.data.TensorDataset(x, y), batch_size=self.batch_size, shuffle=True)
for epoch in range(self.epochs):
    for i, (x_batch, y_batch) in enumerate(batches):
        x_batch = x_batch.cuda()
        y_batch = y_batch.cuda()
        optimizer.zero_grad()
        with torch.cuda.amp.autocast():
            y_pred = model(x_batch).squeeze()
            loss = self.loss_fn(y_pred, y_batch)
        scaler.scale(loss).backward()
        lv = loss.detach().cpu().numpy()
        print(f"Epoch {epoch+1}/{self.epoch}; Batch {i}; Loss: {lv}")
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
```

混合精度训练的一半是`torch.cuda.amp.autocast()`, 这是一个上下文管理器， 以实现从`fp32 -> fp16 `的转换， 具体方法是将前向传播和损失计算包在其中。但不是`pytorch`中所有的操作都支持单精度到半精度的转换。

具体见[Automatic Mixed Precision package - torch.amp — PyTorch master documentation](https://pytorch.org/docs/master/amp.html#autocast-op-reference)

但是在输入有 `fp16`和 `fp32`混合的情况下，这些操作具有向上适配(`up-casting`)规则，以确保它们不会出问题。

常见地， `对数、指数、三角函数、正规函数、离散函数`和(大)和在 `fp16`中是不安全的，必须在 `fp32`中执行。

`torch.cuda.amp.GradScaler()`

<font color=red>**为什么进行放缩?**</font>

在训练期间， 为了防止梯度变小到0， （从高精度到低精度的转换）， 那么缩放就是必要的， 显然主要是放大。最佳的损失乘数得足够高以保留小的梯度， 同时不能太高以至于导致梯度四舍五入之后到inf。

`torch.cuda.amp.GradScaler()`使用的是指数退避， 来解决这个问题， `Gradscaler`以一个小的损失乘数开始， 这个乘数每次都会翻倍， 这种逐渐加倍的行为一直持续到`Gradscaler`包含Inf值的梯度。`Gradscalar`丢弃这批数据（不进行梯度更新）， 然后将损失乘数减半， 并重置其倍增时间。

具体到程序， `Gradscaler`需要对梯度更新计算（检查是否溢出）和优化器（将丢弃的`batches`转换为`no-op`）进行控制， 以实现其操作。这就是为什么`loss.backward()`被`scaler.scale(loss).backeard()`以及`optimizer.step()`被`scale.step(optimizer)`替换的原因。

值得注意是， `Gradscaler()`可以检测并停止`overflows`(上溢)， 但是它不能检测`underflows`(下溢)， 因为0通常是一个合法值。**如果你选择的初始化值太小同时增长间隔太长的话，网络可能会出现发散的情况，因此， 一般会选择一个非常大的初始值**， 常见地，` init_sacle = 2^16`

