### KNN（无监督）

`KNN`:` k`个最近的邻居， 即每个样本都可以用它最接近的`k`个邻居来代表

该算法的思想是： 一个样本与数据集中的k个样本最相似， 如果这k个样本中的大多数属于某一个类别， 则该样本也属于这个类别。

**距离度量**

在选择两个实例相似性时， 一般使用的欧氏距离

Lp距离定义：
$$
dis = \sqrt{\sum^n_{k=1} (x_1 - x_2)^2}
$$
**K值选择**

如果选择较小的k值， 就相当于用较小的邻域中的训练实例进行预测， 学习的**近似误差**会减小， 只有与输入实例较近的训练实例才会对预测结果起作用， 但缺点是学习的**估计误差**会增大， 预测结果会对近邻的实例点敏感。如果邻近的实例点恰巧是噪声， 预测就会出错。换句话说， K值减小就意味着整体模型变复杂， 分的不清楚， 容易过拟合

如果选择较大K值， 就相当于用较大邻域中的训练实例进行预测， 其优点是可以减少学习的**估计误差**， 但**近似误差**会增大，也就是对输入实例预测不准确， K值的增大就意味着整体模型变的简单。

**近似误差：相当于训练集的训练误差**

**测试误差： 相当于测试集的测试误差**

在应用中，K值一般取一个比较小的数值，通常采用交叉验证法来选取最优的K值。

流程：
1） 计算已知类别数据集中的点与当前点之间的距离
2） 按距离递增次序排序
3） 选取与当前点距离最小的k个点
4） 统计前k个点所在的类别出现的频率
5） 返回前k个点出现频率最高的类别作为当前点的预测分类