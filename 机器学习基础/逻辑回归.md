### 逻辑回归（对数几率回归）

为了做分类任务我们可以通过寻找一个单调可微函数将分类任务的真实标记与线性回归模型的预测值联系起来。

<font color=red> 逻辑回归是有监督的</font>

**为什么选用sigmoid函数做作为联系函数？**

【最大熵的角度】

https://zhuanlan.zhihu.com/p/59519202

https://transwarpio.github.io/teaching_ml/2017/08/15/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/

[Maximum Entropy Model最大熵模型 - 简书](https://www.jianshu.com/p/10d778068f70)

LR的本质为二分类下选取了某个特征函数的最大熵模型。

推导出损失函数：可以利用**极大似然**和**信息论**的角度进行推导



[面试题解答：为什么逻辑斯蒂回归的输出值可以作为概率 - 秃头之路的文章 - 知乎]( https://zhuanlan.zhihu.com/p/441128484)



#### 极大似然估计

首先确定概率质量函数（离散的为例）,已知离散型随机变量$y\in\{0,1\}$取值为1和0的概率分别建模为

$p(y=1|\hat{x};\beta)=\frac{e^{\beta^T\hat{x}}}{1+e^{\beta^T\hat{x}}}=p_1(\hat{x};\beta)$

$p(y=0|\hat{x};\beta)=\frac{1}{1+e^{\beta^T\hat{x}}}=p_0(\hat{x};\beta)$

由以上概率取值可推得随机变量$y\in\{0,1\}$的概率质量函数为

$p(y|\hat{x};\beta)=y* p_1(\hat{x};\beta)+(1-y)*p_0(\hat{x};\beta)$

似然函数为：$L(\beta)=\prod^m_{i=1}p(y_i|\hat{x_i};\beta)$

进一步对数似然为：$l(\beta)=lnL(\beta)=\sum^m_{i=1}ln p(y_i|\hat{x_i};\beta)$

$l(\beta)=lnL(\beta)=\sum^m_{i=1}ln y*p_1(\hat{x_i};\beta)+(1-y_i)*p_0(\hat{x_i};\beta)$

带入`p1`和`p0`，我们可以得到最终的`损失函数`：

$l(\beta)=\sum^m_{i=1}(y_i\beta^T\hat{x_i}-ln(1+e^{\beta^T\hat{x_i}}))$

由于损失函数通常为以最小化为优化目标，因此可以将最大化$l(\beta)$等价于最小化$-l(\beta)$



#### 信息论

相对熵（KL散度）：度量两个分布的差异，其典型使用场景是用来度量理想分布$p(x)$和模拟分布$q(x)$之间的差异。

$D_{KL}(p||q)=\sum_xp(x)log_bp(x)-\sum_xp(x)log_bq(x)$

从机器学习三要素中“策略”的角度来说，与理想分布最接近的模拟分布即为最优分布，因此可以通过最小化相对熵这个策略来求最优分布，由于理想分布p(x)是未知但固定的分布，所以$\sum_xp(x)log_bp(x)$为常量，那么最小化相对熵等价于最小化交叉熵

那么单个样本$y_i$的交叉熵为：

$-\sum_xp(x)log_bq(x)$

$-\sum_{y_i}p(y_i)log_bq(y_i) = -p(1)*log_bp_1(\hat{x};\beta)-p(0)*log_bp_0(\hat{x};\beta)=-y_i*log_bp_1(\hat{x};\beta)-(1-y_i)*log_bp_0(\hat{x};\beta)$

换成以e为底

$-y_i*lnp_1(\hat{x};\beta)-(1-y_i)*lnp_0(\hat{x};\beta)$

全体训练样本的交叉熵为

$\sum^m_{i=1}[-y_ilnp_1(\hat{x_i};\beta)-(1-y_i)lnp_0(\hat{x_i};\beta)]$

类似地，带入p1和p0

$=\sum^m_{i=1}[-y_iln(e^{\beta^T\hat{x_i}}-ln(\frac{1}{1+e^{\beta^T\hat{x_i}}}))]=\sum^m_{i=1}(-y_i\beta^T\hat{x_i}+ln(1+e^{\beta^T\hat{x_i}}))$

同样也可以推导出相同的结果，该式为高阶可导连续凸函数，可以根据**梯度下降法**、**牛顿法**等进行求解最优解。