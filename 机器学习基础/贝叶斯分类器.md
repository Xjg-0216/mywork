### 贝叶斯分类器

##### 贝叶斯决策论

贝叶斯决策论是概率框架下实施决策的基本方法， 对分类任务来说， 在所有相关概率都已知的理性情形下， 贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

假设有N种可能的类别标记，即$y=\{c_1, c_2,c_3,...,c_N\}$,$\lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失，基于后验概率$P(c_i|x)$可获得将样本x分类为$c_i$所产生的期望损失，即在样本x上的“条件风险”

$R(c_i|x)=\sum^N_{j=1}\lambda_{ij}P(c_j|x)$

全部样本构成的总体风险为：

$R(h)=E_x[R(h(x)|x)]$

其中，h为分类器模型，显然，分类效果越准确的h，其条件风险和总体风险也越小。

贝叶斯判定准则：为最小化总体风险R(h)，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记，即：

$h^*(x)=argmin_{c\in y}P(c|x)$

此时，$h^*$称为贝叶斯最优分类器

具体地，若目标是最小化分类错误率，误判损失$\lambda_{ij}$在i=j的时候为0，其他情况下为1

此时，单个样本x的期望损失（条件风险）为

$R(c_i|x)=\sum^N_{j=1}\lambda_{ij}P(c_j|x)$

又因为$\sum^N_{j=1}P(c_j|x)=1$,则$R(c_i|x)=1-P(c_i|x)$

于是，按照上述的推导，最小化分类错误率的贝叶斯最有分类器等价于 **最大化后验概率**

从贝叶斯决策论的角度：机器学习所要做的就是基于有限的训练样本集尽可能准确地估计出后验概率$P(c|x)$

从机器学习自己的角度：给定一个样本x， 求一个能准确分类x的f(x)，其有些算法可以看作是对后验概率建模P(c|x)，而有些算法则是纯粹完成样本分类（SVM）

************

判别式模型：给定x, 直接建模P(c|x)来预测c

生成式模型：先对联合概率P(x,c)建模，然后再由此推导出P(c|x)

******

对于生成式模型，其建模思路为：

$P(c|x)=\frac{P(x,c)}{P(x)}$

再根据贝叶斯定理，上式可恒等变形为：

$P(c|x)=\frac{P(c)P(x|c)}{P(x)}$

其中，P(c)是类先验概率，P(x|c)是样本x相对于类别标记c的类条件概率(似然函数)，P(x)是用于归一化的证据因子

##### 朴素贝叶斯分类器

基于贝叶斯公式来估计后验概率的主要困难在于：似然函数是所有属性的联合概率。

**属性条件独立性假设：对已知类别，假设所有属性（特征）相互独立**

$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod^d_{i=1}P(x_i|c)$

其中，d为属性数目,$x_i$为x在第i个属性上的取值，基于贝叶斯判定准则：

$h^*(x)=argmax_{c\in y}P(c|x)=argmax_{c\in y}\frac{P(c)}{P(x)}\prod^d_{i=1}P(x_i|c)$

由于对所有类别来说P(x)都相同，所以P(x)视为常量可以略去，则朴素贝叶斯最优分类器为：

$h_{nb}(x)=argmax_{c\in y}P(c)\prod^d_{i=1}P(x_i|c)$

**从而朴素贝叶斯最优分类器等价于含$\theta$参数的极大似然估计**

##### 半朴素贝叶斯分类器

为了降低贝叶斯公式中估计后验概率的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立，半朴素贝叶斯分类器的基本想法是：**适当考虑一部分属性间的相互依赖信息，从而既不需进行完全概率计算，又不至于彻底忽略了比较强的属性依赖关系**，独依赖估计是半朴素贝叶斯分类器最常见的策略：假设每个属性在类别之外最多仅依赖于一个其他属性，即：

$P(c|x) = P(c)\prod^d_{i=1}P(x_i|c,pa_i)$