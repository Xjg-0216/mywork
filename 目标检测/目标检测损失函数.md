### 目标检测损失函数

#### 1、faster-RCNN

$$
L(\{p_i\},\{t_i\}) = \frac{1}{N_{cls}}\sum_i L_{cls}(p_i, p^*_i) + \lambda \frac{1}{N_{reg}} \sum_i p^*_iL_{reg}(t_i, t^*_i)
$$



$p_i$表示第`i`个`anchor`预测为目标的概率， $p^*$当为正样本时为`1`， 当为负样本时为`0`

$L_{cls}$为`二值交叉熵损失`， 值得注意的是， 交叉熵损失：

1、针对多分类问题， （softmax输出， 所有输出概率和为1）
$$
H = - \sum_i o^*_i log(o_i)
$$
2、针对二分类问题，（sigmoid输出， 每个输出节点之间互不相干）
$$
H = -\frac{1}{N}[o^*_i~log ~o_i + (1 - o^*_i~log(1 - o_i))]
$$
**用tensor实现二分类交叉熵损失**

```python
N = 3 * pow(10, 3)
torch.random.manual_seed(420)
x = torch.rand((N, 4), dtype=torch.float32)
w = torch.rand((4, 1), dtype=torch.float32)
y = torch.randint((0, 2), size=(N,1), dtype=torch.float32)
zhat = torch.mm(x, w)
sigma = torch.sigmoid(zhat)
loss = (-1/N) * torch.sum(y * torch.log(sigma) + (1 - y) * torch.log(1- sigma))
```



**用pytorch中类实现二分类交叉熵损失**

`BCEWithLogitsLoss `以及 `BCELoss`

其中`BCEWithLogitsLoss`自带`sigmoid`函数， 只需要输入预测值`zhat`即可

对于`BCELoss`来说， 需要输入激活函数激活后的`sigmma`

```python
N =  3 * pow(10, 6)
torch.random.manual_seed(420)
x = torch.rand((N, 4), dtype=torch.float32)
w = torch.rand((4, 1), dtype=torch.float32)
y = torch.randint((0, 2), size=(N,1), dtype=torch.float32)
zhat = torch.mm(x, w)
criterion = nn.BCEWithLogitsLoss()
loss = criterion(zhat, y)
```

```python
N = 3 * pow(10, 6)
torch,random.manual_seed(420)
x = torch.rand((N, 4), dtype=torch.float32)
w = torch.rand((4, 1), dtype=torch.float32)
y = torch.randint((0, 2, size = (N, 1), dtype= torch.float32)
zhat = torch.mm(x, w)
sigma = torch.sigmoid(zhat)
cirterion = nn.BCELoss()
loss = cirterion(sigma, y)
```

**用pytorch 类实现多分类交叉熵损失函数**



需要用到`one-hot编码`

[one-hot 编码](./one-hot编码.md)

两种方法：

`LogSoftmax和NLLLoss`

`CrossEntropyLoss`

```python
N = 3 * pow(10, 2)
torch.rand.manual_seed(420)
x = torch.rand((N, 4), dtype= torch.float32)
w = torch.rand((4, 3), dtype= torch.float32)
zhat = torch.mm(x, w)
y = torch.randint((0, 4), size= (N, ), dtype=torch.float32)
logsm = nn.Logsoftmax(dim = 1)
logsigma = logsm(zhat)
criterion = nn.NLLLoss()
# 由于交叉熵损失需要将标签转换为one-hot形式， 因此不接受浮点数作为标签的输入
cirterion(logsigma, y.long())
```

直接调用`CrossEntropyLoss`

```python
N = 3 * pow(10, 2)
torch.rand.manual_seed(420)
x = torch.rand((N, 4), dtype= torch.float32)
w = torch.rand((4, 3), dtype= torch.float32)
zhat = torch.mm(x, w)
y = torch.randint((0, 4), size= (N, ), dtype=torch.float32)
cirterion = nn.CrossEntropyLoss()
loss = cirterion(zhat, y.long())
```





$L_{reg}$为边界框回归损失， 采用的是`smoothl1`损失
$$
L_{cls} = - [p^*_i log(p_i) + (1- p^*_i)log(1-p_i)]​
$$

$$
L_{reg}(t_i, t^*_i) = \sum_i smooth_{L1}(t_i - t^*_i)
$$

$$
smooth_{L1}(x) = 
\left\{
\begin{matrix}
0.5x^2,   if |x| < 1 \\
|x| - 0.5 , otherwise\\
\end{matrix}
\right.
$$
**SmoothL1损失**

```python
def smoothl1_loss(input, target, sigma):
    beta = 1. 
    diff = torch.abs(input-target)
    cond = diff < beta
    loss = torch.where(cond, 0.5 * diff ** 2 , diff - 0.5)
    return torch.sum(loss, dim=1)
```

